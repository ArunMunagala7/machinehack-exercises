{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026aed78",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:09.537548Z",
     "iopub.status.busy": "2023-02-07T02:38:09.537041Z",
     "iopub.status.idle": "2023-02-07T02:38:09.551848Z",
     "shell.execute_reply": "2023-02-07T02:38:09.550319Z"
    },
    "papermill": {
     "duration": 0.033877,
     "end_time": "2023-02-07T02:38:09.554860",
     "exception": false,
     "start_time": "2023-02-07T02:38:09.520983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "907f0c2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:09.582724Z",
     "iopub.status.busy": "2023-02-07T02:38:09.581842Z",
     "iopub.status.idle": "2023-02-07T02:38:10.872372Z",
     "shell.execute_reply": "2023-02-07T02:38:10.871081Z"
    },
    "papermill": {
     "duration": 1.308113,
     "end_time": "2023-02-07T02:38:10.875698",
     "exception": false,
     "start_time": "2023-02-07T02:38:09.567585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2710c8bd",
   "metadata": {
    "papermill": {
     "duration": 0.012232,
     "end_time": "2023-02-07T02:38:10.902137",
     "exception": false,
     "start_time": "2023-02-07T02:38:10.889905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As we could understand that text analytics is a process in NLP for analyzing textual data in order to make suitable insights. It analyzes the textual data for frequently occurring words and visualises the frequently occurring words through interpretable plots. The process of text analysis also helps us to understand the textual data better and in the later stage build any relevant NLP models as required. \n",
    "\n",
    "In this section, we will cover the python implementation of the important steps in text analytics. So let us see what are all the different processes included in the process of text analytics. They are:-\n",
    "\n",
    "1.Data Reading and Manipulation\n",
    "2.Regex operations on the textual data\n",
    "3.Tokenization and lower case conversion\n",
    "4.Stopword removal\n",
    "5.Stemming the textual data\n",
    "6.Lemmatization\n",
    "7.Visualizing the Cleaned textual data with WordClouds\n",
    "8.CountVectorizer for feature extraction\n",
    "9.TF-IDF\n",
    "10.Analyzing the common words in the textual data with grams\n",
    "Let’s start covering the implementation of each of the steps one by one.\n",
    "\n",
    "**1. Data Reading and Manipulation**\n",
    "\n",
    "Step-1: Read and load the data\n",
    "\n",
    "Let us load the CSV file into the working environment using the Pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091d0d1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:10.929179Z",
     "iopub.status.busy": "2023-02-07T02:38:10.928061Z",
     "iopub.status.idle": "2023-02-07T02:38:11.215696Z",
     "shell.execute_reply": "2023-02-07T02:38:11.214638Z"
    },
    "papermill": {
     "duration": 0.303869,
     "end_time": "2023-02-07T02:38:11.218271",
     "exception": false,
     "start_time": "2023-02-07T02:38:10.914402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('https://raw.githubusercontent.com/analyticsindiamagazine/MocksDatasets/main/NLP-movie-review.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad6733",
   "metadata": {
    "papermill": {
     "duration": 0.011888,
     "end_time": "2023-02-07T02:38:11.242634",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.230746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now as we have loaded the dataset into the working environment let us visualize the first few entries of the dataset using the head() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dda8db77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:11.269362Z",
     "iopub.status.busy": "2023-02-07T02:38:11.268541Z",
     "iopub.status.idle": "2023-02-07T02:38:11.289495Z",
     "shell.execute_reply": "2023-02-07T02:38:11.288294Z"
    },
    "papermill": {
     "duration": 0.037667,
     "end_time": "2023-02-07T02:38:11.292563",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.254896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>reaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Service is friendly and inviting.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Awesome service and food.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Waitress was a little slow in service.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Come hungry, leave happy and stuffed!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Horrible - don't waste your time and money.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        review  reaction\n",
       "0            Service is friendly and inviting.         1\n",
       "1                    Awesome service and food.         1\n",
       "2       Waitress was a little slow in service.         0\n",
       "3        Come hungry, leave happy and stuffed!         1\n",
       "4  Horrible - don't waste your time and money.         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()## First 5 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81646dc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:11.319891Z",
     "iopub.status.busy": "2023-02-07T02:38:11.318564Z",
     "iopub.status.idle": "2023-02-07T02:38:11.329866Z",
     "shell.execute_reply": "2023-02-07T02:38:11.328715Z"
    },
    "papermill": {
     "duration": 0.027353,
     "end_time": "2023-02-07T02:38:11.332207",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.304854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>reaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>You won't be disappointed.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>What did bother me, was the slow service.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>Those burgers were amazing.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>I have watched their prices inflate, portions ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>The ripped banana was not only ripped, but pet...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>This was my first time and I can't wait until ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Great service and food.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>I paid the bill but did not tip because I felt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The one down note is the ventilation could use...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>I really enjoyed eating here.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  reaction\n",
       "990                         You won't be disappointed.         1\n",
       "991          What did bother me, was the slow service.         0\n",
       "992                        Those burgers were amazing.         1\n",
       "993  I have watched their prices inflate, portions ...         0\n",
       "994  The ripped banana was not only ripped, but pet...         0\n",
       "995  This was my first time and I can't wait until ...         1\n",
       "996                            Great service and food.         1\n",
       "997  I paid the bill but did not tip because I felt...         0\n",
       "998  The one down note is the ventilation could use...         0\n",
       "999                      I really enjoyed eating here.         1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57969cd7",
   "metadata": {
    "papermill": {
     "duration": 0.012379,
     "end_time": "2023-02-07T02:38:11.358000",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.345621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now as we have visualized the dataset in the working environment let us validate the number of rows and columns present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cb44fba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:11.386315Z",
     "iopub.status.busy": "2023-02-07T02:38:11.385628Z",
     "iopub.status.idle": "2023-02-07T02:38:11.392369Z",
     "shell.execute_reply": "2023-02-07T02:38:11.390904Z"
    },
    "papermill": {
     "duration": 0.024424,
     "end_time": "2023-02-07T02:38:11.395554",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.371130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows and 2 columns in the dataset\n"
     ]
    }
   ],
   "source": [
    "print('There are {} rows and {} columns in the dataset'.format(df.shape[0],df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5449cc8d",
   "metadata": {
    "papermill": {
     "duration": 0.012727,
     "end_time": "2023-02-07T02:38:11.422643",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.409916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So here we can see that there are 1000 rows and 2 columns in the dataset.\n",
    "\n",
    "In the reaction column the reaction is given values of 0 and 1 and for easy interpretation let us manipulate the value of 1 as positive and 0 as negative as shown below.\n",
    "\n",
    "**Step-2: Manipulating the data as required**\n",
    "\n",
    "For manipulating the data as required we can use the replace function of pandas dataframe where at first the value to replace is mentioned and the value to be imputed newly is mentioned and to make permanent changes in the dataframe the inplace parameter is set as True as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "507df2a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:11.452453Z",
     "iopub.status.busy": "2023-02-07T02:38:11.451743Z",
     "iopub.status.idle": "2023-02-07T02:38:11.466163Z",
     "shell.execute_reply": "2023-02-07T02:38:11.464873Z"
    },
    "papermill": {
     "duration": 0.032604,
     "end_time": "2023-02-07T02:38:11.469002",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.436398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['reaction'].replace(to_replace=1,value='positive',inplace=True)\n",
    "df['reaction'].replace(to_replace=0,value='negative',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd68405",
   "metadata": {
    "papermill": {
     "duration": 0.012928,
     "end_time": "2023-02-07T02:38:11.494668",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.481740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now as we have transformed the values appropriately let us visualize the distribution of the target variable through a count plot as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e83c883a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:11.522078Z",
     "iopub.status.busy": "2023-02-07T02:38:11.521645Z",
     "iopub.status.idle": "2023-02-07T02:38:11.745793Z",
     "shell.execute_reply": "2023-02-07T02:38:11.744607Z"
    },
    "papermill": {
     "duration": 0.240924,
     "end_time": "2023-02-07T02:38:11.748565",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.507641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df.reaction)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c765c",
   "metadata": {
    "papermill": {
     "duration": 0.013346,
     "end_time": "2023-02-07T02:38:11.775099",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.761753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Regex operations on the textual data**\n",
    "\n",
    "Regex is one of the modules that python offers to analyze the test data for some regular string expressions. Using Regex the required characters can be filtered out from the textual data through suitable preprocessing and using the sub() module of Regex all the required patterns of the textual data can be fetched.\n",
    "\n",
    "**Step - 1: Importing the required module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc4408d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:11.804874Z",
     "iopub.status.busy": "2023-02-07T02:38:11.803727Z",
     "iopub.status.idle": "2023-02-07T02:38:11.809272Z",
     "shell.execute_reply": "2023-02-07T02:38:11.807923Z"
    },
    "papermill": {
     "duration": 0.022844,
     "end_time": "2023-02-07T02:38:11.812002",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.789158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93796285",
   "metadata": {
    "papermill": {
     "duration": 0.012989,
     "end_time": "2023-02-07T02:38:11.838237",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.825248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So once the Regex library is loaded into the working environment let us use this library to perform text analysis.\n",
    "\n",
    "**Step-2: Using Regex to extract only the required text from the data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1f859ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:11.866177Z",
     "iopub.status.busy": "2023-02-07T02:38:11.865748Z",
     "iopub.status.idle": "2023-02-07T02:38:11.877553Z",
     "shell.execute_reply": "2023-02-07T02:38:11.876114Z"
    },
    "papermill": {
     "duration": 0.028692,
     "end_time": "2023-02-07T02:38:11.880057",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.851365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_doc = df.review.apply(lambda x: re.sub(\"[^a-zA-Z]\",\" \",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38cca08f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:11.908751Z",
     "iopub.status.busy": "2023-02-07T02:38:11.907995Z",
     "iopub.status.idle": "2023-02-07T02:38:11.915637Z",
     "shell.execute_reply": "2023-02-07T02:38:11.914435Z"
    },
    "papermill": {
     "duration": 0.024687,
     "end_time": "2023-02-07T02:38:11.918202",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.893515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "995    This was my first time and I can t wait until ...\n",
       "996                              Great service and food \n",
       "997    I paid the bill but did not tip because I felt...\n",
       "998    The one down note is the ventilation could use...\n",
       "999                        I really enjoyed eating here \n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_doc.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e7068",
   "metadata": {
    "papermill": {
     "duration": 0.013062,
     "end_time": "2023-02-07T02:38:11.944646",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.931584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here basically a lambda function is written to extract only alphabets in the textual data and filter out all other characters such as #,@, and many more which are most commonly used in textual data. So here only alphabets a-z and A-Z alphabets are extracted from the text data.\n",
    "\n",
    "**Step-3: Observing the data before and after Regex operations**\n",
    "\n",
    "So let us observe the data by using simple print statements to analyze the data before and after Regex operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9b10bc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:11.973554Z",
     "iopub.status.busy": "2023-02-07T02:38:11.973139Z",
     "iopub.status.idle": "2023-02-07T02:38:11.979443Z",
     "shell.execute_reply": "2023-02-07T02:38:11.978340Z"
    },
    "papermill": {
     "duration": 0.024439,
     "end_time": "2023-02-07T02:38:11.982689",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.958250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before regex operation Horrible - don't waste your time and money.\n",
      "\n",
      "Data after regex operation Horrible   don t waste your time and money \n"
     ]
    }
   ],
   "source": [
    "print('Data before regex operation {}'.format(df['review'][4]))\n",
    "print()\n",
    "print('Data after regex operation {}'.format(text_doc[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb5ce9",
   "metadata": {
    "papermill": {
     "duration": 0.013022,
     "end_time": "2023-02-07T02:38:12.010175",
     "exception": false,
     "start_time": "2023-02-07T02:38:11.997153",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So here after the Regex operation, we can observe that the (-) character is removed from the original text data.\n",
    "\n",
    "**Tokenization and lower case conversion**\n",
    "\n",
    "Tokenization is the process of splitting the entire textual data into small pieces of text called tokens wherein each of the first columns of the textual data will be coveted to a list of tokens split with a comma (,) operator.\n",
    "\n",
    "**Step - 1: Importing the required modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cb1e429",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:12.038926Z",
     "iopub.status.busy": "2023-02-07T02:38:12.037833Z",
     "iopub.status.idle": "2023-02-07T02:38:12.865416Z",
     "shell.execute_reply": "2023-02-07T02:38:12.864153Z"
    },
    "papermill": {
     "duration": 0.844825,
     "end_time": "2023-02-07T02:38:12.868159",
     "exception": false,
     "start_time": "2023-02-07T02:38:12.023334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b091fcbd",
   "metadata": {
    "papermill": {
     "duration": 0.013037,
     "end_time": "2023-02-07T02:38:12.894385",
     "exception": false,
     "start_time": "2023-02-07T02:38:12.881348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So here the required module for tokenization is made available in the working environment.\n",
    "\n",
    "**Step - 2: Creating a tokenization instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "263cf33a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:12.922472Z",
     "iopub.status.busy": "2023-02-07T02:38:12.922056Z",
     "iopub.status.idle": "2023-02-07T02:38:12.927389Z",
     "shell.execute_reply": "2023-02-07T02:38:12.926222Z"
    },
    "papermill": {
     "duration": 0.022655,
     "end_time": "2023-02-07T02:38:12.930218",
     "exception": false,
     "start_time": "2023-02-07T02:38:12.907563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenize=RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564471db",
   "metadata": {
    "papermill": {
     "duration": 0.0133,
     "end_time": "2023-02-07T02:38:12.956816",
     "exception": false,
     "start_time": "2023-02-07T02:38:12.943516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here in the tokenization instance being created (\\w+) instance is being used to match one or more occurring words which may either be a-z or A-Z. So now using this instance let us see how to perform tokenization.\n",
    "\n",
    "**Step - 3: Performing Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1426ce46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:12.985758Z",
     "iopub.status.busy": "2023-02-07T02:38:12.985334Z",
     "iopub.status.idle": "2023-02-07T02:38:12.997805Z",
     "shell.execute_reply": "2023-02-07T02:38:12.996445Z"
    },
    "papermill": {
     "duration": 0.031162,
     "end_time": "2023-02-07T02:38:13.001073",
     "exception": false,
     "start_time": "2023-02-07T02:38:12.969911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens=text_doc.apply(lambda x:tokenize.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceec885",
   "metadata": {
    "papermill": {
     "duration": 0.013384,
     "end_time": "2023-02-07T02:38:13.029109",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.015725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A lambda function is used to perform tokenization on the entire textual data by calling the inbuilt function of RegexpTokenizer. Let us visualize the tokenized data using a print statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b42f94fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:13.058121Z",
     "iopub.status.busy": "2023-02-07T02:38:13.057711Z",
     "iopub.status.idle": "2023-02-07T02:38:13.065637Z",
     "shell.execute_reply": "2023-02-07T02:38:13.064354Z"
    },
    "papermill": {
     "duration": 0.025939,
     "end_time": "2023-02-07T02:38:13.068746",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.042807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               [Service, is, friendly, and, inviting]\n",
      "1                        [Awesome, service, and, food]\n",
      "2        [Waitress, was, a, little, slow, in, service]\n",
      "3           [Come, hungry, leave, happy, and, stuffed]\n",
      "4    [Horrible, don, t, waste, your, time, and, money]\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6602f19",
   "metadata": {
    "papermill": {
     "duration": 0.013172,
     "end_time": "2023-02-07T02:38:13.095429",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.082257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So this is how the entire text data is split into small pieces using the process of Tokenization. So now we have to convert all the tokens of the text data into lower case.\n",
    "\n",
    "**Step-4: Lower case conversion of the tokenized data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4cf969c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:13.124802Z",
     "iopub.status.busy": "2023-02-07T02:38:13.124403Z",
     "iopub.status.idle": "2023-02-07T02:38:13.134273Z",
     "shell.execute_reply": "2023-02-07T02:38:13.132928Z"
    },
    "papermill": {
     "duration": 0.028041,
     "end_time": "2023-02-07T02:38:13.136831",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.108790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lower=[list(map(lambda x:x.lower(),l)) for l in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3885bd68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:13.165839Z",
     "iopub.status.busy": "2023-02-07T02:38:13.165439Z",
     "iopub.status.idle": "2023-02-07T02:38:13.171576Z",
     "shell.execute_reply": "2023-02-07T02:38:13.170335Z"
    },
    "papermill": {
     "duration": 0.024729,
     "end_time": "2023-02-07T02:38:13.174883",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.150154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['service', 'is', 'friendly', 'and', 'inviting'], ['awesome', 'service', 'and', 'food'], ['waitress', 'was', 'a', 'little', 'slow', 'in', 'service'], ['come', 'hungry', 'leave', 'happy', 'and', 'stuffed'], ['horrible', 'don', 't', 'waste', 'your', 'time', 'and', 'money']]\n"
     ]
    }
   ],
   "source": [
    "print(lower[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61eb7dd",
   "metadata": {
    "papermill": {
     "duration": 0.013729,
     "end_time": "2023-02-07T02:38:13.203061",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.189332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here a lambda function is used to convert all the tokens in the dataset to lower case and the converted document can be verified through a print statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21eef9d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:13.233616Z",
     "iopub.status.busy": "2023-02-07T02:38:13.233206Z",
     "iopub.status.idle": "2023-02-07T02:38:13.239654Z",
     "shell.execute_reply": "2023-02-07T02:38:13.238500Z"
    },
    "papermill": {
     "duration": 0.026006,
     "end_time": "2023-02-07T02:38:13.243164",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.217158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text document: Awesome service and food \n",
      "\n",
      "Lower case converted document: ['awesome', 'service', 'and', 'food']\n"
     ]
    }
   ],
   "source": [
    "print('Original text document: {}'.format(text_doc[1]))\n",
    "print()\n",
    "print('Lower case converted document: {}'.format(lower[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646b494e",
   "metadata": {
    "papermill": {
     "duration": 0.01329,
     "end_time": "2023-02-07T02:38:13.270990",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.257700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So in this output generated we can observe the differences between the original textual data and the lower case converted data\n",
    "\n",
    "**Stopword removal**\n",
    "\n",
    "Stopwords are also termed as common words that would occur in textual data and it is very important to remove them in the process of text analysis or preprocessing as it would give more weightage to the actual text data utilized at that particular instance.\n",
    "\n",
    "**Step - 1: Downloading the stopwords from the nltk package**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fcc429a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:13.300482Z",
     "iopub.status.busy": "2023-02-07T02:38:13.300009Z",
     "iopub.status.idle": "2023-02-07T02:38:13.433848Z",
     "shell.execute_reply": "2023-02-07T02:38:13.432053Z"
    },
    "papermill": {
     "duration": 0.15225,
     "end_time": "2023-02-07T02:38:13.436846",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.284596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f39d279",
   "metadata": {
    "papermill": {
     "duration": 0.013313,
     "end_time": "2023-02-07T02:38:13.464005",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.450692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So here the pretrained common stopwords of the nltk library is downloaded to the working environment.\n",
    "\n",
    "**Step-2: Importing the required library for removing stopwords**\n",
    "\n",
    "From the nltk package, the corpus library is utilized to retrieve the stopwords library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffa0b4dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:13.494118Z",
     "iopub.status.busy": "2023-02-07T02:38:13.493022Z",
     "iopub.status.idle": "2023-02-07T02:38:13.499257Z",
     "shell.execute_reply": "2023-02-07T02:38:13.497768Z"
    },
    "papermill": {
     "duration": 0.024683,
     "end_time": "2023-02-07T02:38:13.502192",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.477509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29744922",
   "metadata": {
    "papermill": {
     "duration": 0.014157,
     "end_time": "2023-02-07T02:38:13.530821",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.516664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Step-3: Creating an instance for common stopwords in the English language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "148fe37d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:13.561298Z",
     "iopub.status.busy": "2023-02-07T02:38:13.560769Z",
     "iopub.status.idle": "2023-02-07T02:38:13.569999Z",
     "shell.execute_reply": "2023-02-07T02:38:13.569077Z"
    },
    "papermill": {
     "duration": 0.027393,
     "end_time": "2023-02-07T02:38:13.572572",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.545179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b33909",
   "metadata": {
    "papermill": {
     "duration": 0.013475,
     "end_time": "2023-02-07T02:38:13.599695",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.586220",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let us visualize the top 20 stopwords present in the nltk library through slicing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16de8834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:13.631542Z",
     "iopub.status.busy": "2023-02-07T02:38:13.630888Z",
     "iopub.status.idle": "2023-02-07T02:38:13.638593Z",
     "shell.execute_reply": "2023-02-07T02:38:13.637415Z"
    },
    "papermill": {
     "duration": 0.026674,
     "end_time": "2023-02-07T02:38:13.640961",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.614287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b3d861",
   "metadata": {
    "papermill": {
     "duration": 0.013467,
     "end_time": "2023-02-07T02:38:13.669214",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.655747",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here in the output, the first 20 stopwords are obtained that are present in the nltk module.\n",
    "\n",
    "**Step-4: Removing the word not present in the list of nltk common stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f560fb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:13.699169Z",
     "iopub.status.busy": "2023-02-07T02:38:13.698536Z",
     "iopub.status.idle": "2023-02-07T02:38:13.703738Z",
     "shell.execute_reply": "2023-02-07T02:38:13.702686Z"
    },
    "papermill": {
     "duration": 0.02321,
     "end_time": "2023-02-07T02:38:13.706518",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.683308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords.remove('not')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010f26d",
   "metadata": {
    "papermill": {
     "duration": 0.013365,
     "end_time": "2023-02-07T02:38:13.733919",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.720554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The stopwords in the NLTK module are removed by using the simple remove() function of the list.\n",
    "\n",
    "**Step-5:Handling the stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62170ec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:13.763486Z",
     "iopub.status.busy": "2023-02-07T02:38:13.762714Z",
     "iopub.status.idle": "2023-02-07T02:38:13.793245Z",
     "shell.execute_reply": "2023-02-07T02:38:13.792196Z"
    },
    "papermill": {
     "duration": 0.048292,
     "end_time": "2023-02-07T02:38:13.795797",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.747505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop=[]\n",
    "for line in lower:\n",
    " words=[]\n",
    " for word in line:\n",
    "   if word not in stopwords:\n",
    "     words.append(word)\n",
    " stop.append(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb83d10",
   "metadata": {
    "papermill": {
     "duration": 0.013284,
     "end_time": "2023-02-07T02:38:13.822686",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.809402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here an empty list named stop is created to append the words that are not present in the standard list of the nltk module where the stopwords will be removed from the lower case converted data.\n",
    "\n",
    "**Step-6: Analyzing the data before and after stopword removal**\n",
    "\n",
    "Here is a simple print statement to analyze the textual data before and after removing the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "222e85e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:13.853586Z",
     "iopub.status.busy": "2023-02-07T02:38:13.852992Z",
     "iopub.status.idle": "2023-02-07T02:38:13.860442Z",
     "shell.execute_reply": "2023-02-07T02:38:13.858203Z"
    },
    "papermill": {
     "duration": 0.027453,
     "end_time": "2023-02-07T02:38:13.864854",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.837401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text column with stopword ['i', 'really', 'enjoyed', 'eating', 'here']\n",
      "\n",
      "Text column after removing stopword ['really', 'enjoyed', 'eating']\n"
     ]
    }
   ],
   "source": [
    "print('Text column with stopword',lower[-1])\n",
    "print()\n",
    "print('Text column after removing stopword',stop[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4da287",
   "metadata": {
    "papermill": {
     "duration": 0.014599,
     "end_time": "2023-02-07T02:38:13.895990",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.881391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So here in the output, we can see that the common stop words of the English language like ‘i’ and ‘here’ are removed.\n",
    "\n",
    "**5. Stemming the textual data**\n",
    "\n",
    "Stemming is the process of just removing the last characters of each of the lower case converted characters using some standard NLTK packages. Here let us see how to use the LanCasterStemmer to perform the stemming operation.\n",
    "\n",
    "**Step-1: Importing the stemming package from NLTK**\n",
    "\n",
    "Here the Lancaster Stemmer library is being imported into the working environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3795197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:13.927352Z",
     "iopub.status.busy": "2023-02-07T02:38:13.926919Z",
     "iopub.status.idle": "2023-02-07T02:38:13.932677Z",
     "shell.execute_reply": "2023-02-07T02:38:13.931371Z"
    },
    "papermill": {
     "duration": 0.024274,
     "end_time": "2023-02-07T02:38:13.935364",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.911090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a63a7f",
   "metadata": {
    "papermill": {
     "duration": 0.013706,
     "end_time": "2023-02-07T02:38:13.963186",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.949480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Step - 2: Performing the stemming operation**\n",
    "\n",
    "Here an empty list named stem is created and the stem function of LanCasterStemmer is used to chop off the last occurring character and the stem word will be appended to the empty list being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b05655a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:13.993690Z",
     "iopub.status.busy": "2023-02-07T02:38:13.992898Z",
     "iopub.status.idle": "2023-02-07T02:38:14.636182Z",
     "shell.execute_reply": "2023-02-07T02:38:14.634504Z"
    },
    "papermill": {
     "duration": 0.661967,
     "end_time": "2023-02-07T02:38:14.639118",
     "exception": false,
     "start_time": "2023-02-07T02:38:13.977151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stem=[]\n",
    "for line in stop:\n",
    " words=[]\n",
    " for word in line:\n",
    "   words.append(LancasterStemmer().stem(word))\n",
    " stem.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1417541a",
   "metadata": {
    "papermill": {
     "duration": 0.014986,
     "end_time": "2023-02-07T02:38:14.668889",
     "exception": false,
     "start_time": "2023-02-07T02:38:14.653903",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So here only the stemmed words are appended to the stem list being created.\n",
    "\n",
    "**Step-3: Analyzing the data before and after stemming**\n",
    "\n",
    "Here the data before and after performing stemming is analyzed through a simple print statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "434c56b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:14.699900Z",
     "iopub.status.busy": "2023-02-07T02:38:14.699222Z",
     "iopub.status.idle": "2023-02-07T02:38:14.704701Z",
     "shell.execute_reply": "2023-02-07T02:38:14.703819Z"
    },
    "papermill": {
     "duration": 0.023056,
     "end_time": "2023-02-07T02:38:14.707147",
     "exception": false,
     "start_time": "2023-02-07T02:38:14.684091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text column without stemming : ['awesome', 'service', 'and', 'food']\n",
      "\n",
      "Text column after stemming : ['awesom', 'serv', 'food']\n"
     ]
    }
   ],
   "source": [
    "print('Text column without stemming : {}'.format(lower[1]))\n",
    "print()\n",
    "print('Text column after stemming : {}'.format(stem[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f6b11e",
   "metadata": {
    "papermill": {
     "duration": 0.015334,
     "end_time": "2023-02-07T02:38:14.738970",
     "exception": false,
     "start_time": "2023-02-07T02:38:14.723636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here in the output, we can see that after applying stemming to the lower case converted test data that words like service are converted to serve and awesome to awesom\n",
    "\n",
    "*6. Lemmatization*\n",
    "\n",
    "Lemmatization is one such process of text analytics where the context or the meaning of each of the lower case converted tokens is considered and the lemmatization process occurs. So let us see how to utilize the LanCasterStemmer of the NLTK module to implement lemmatization.\n",
    "\n",
    "**Step-1: Importing the required library**\n",
    "\n",
    "Here the required package of Lemmatization is imported into the working environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcab73eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:14.769805Z",
     "iopub.status.busy": "2023-02-07T02:38:14.769168Z",
     "iopub.status.idle": "2023-02-07T02:38:14.773730Z",
     "shell.execute_reply": "2023-02-07T02:38:14.772586Z"
    },
    "papermill": {
     "duration": 0.02255,
     "end_time": "2023-02-07T02:38:14.776339",
     "exception": false,
     "start_time": "2023-02-07T02:38:14.753789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57efb9",
   "metadata": {
    "papermill": {
     "duration": 0.013296,
     "end_time": "2023-02-07T02:38:14.804614",
     "exception": false,
     "start_time": "2023-02-07T02:38:14.791318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Step-2: Creating a lemmatization instance**\n",
    "\n",
    "Here an instance for WordNetLemmatizer is created as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c468201",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:14.834280Z",
     "iopub.status.busy": "2023-02-07T02:38:14.833872Z",
     "iopub.status.idle": "2023-02-07T02:38:14.839442Z",
     "shell.execute_reply": "2023-02-07T02:38:14.837897Z"
    },
    "papermill": {
     "duration": 0.024107,
     "end_time": "2023-02-07T02:38:14.842372",
     "exception": false,
     "start_time": "2023-02-07T02:38:14.818265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_net_lem=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d54051f",
   "metadata": {
    "papermill": {
     "duration": 0.013578,
     "end_time": "2023-02-07T02:38:14.870861",
     "exception": false,
     "start_time": "2023-02-07T02:38:14.857283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Step-3: Performing the lemmatization operation**\n",
    "\n",
    "Here an empty list is created to append the lemmatized word by the lemmatization instance by using the inbuilt function of lemmatization named lemmatize as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e09caa1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:14.901649Z",
     "iopub.status.busy": "2023-02-07T02:38:14.901262Z",
     "iopub.status.idle": "2023-02-07T02:38:16.956564Z",
     "shell.execute_reply": "2023-02-07T02:38:16.955409Z"
    },
    "papermill": {
     "duration": 2.07409,
     "end_time": "2023-02-07T02:38:16.959287",
     "exception": false,
     "start_time": "2023-02-07T02:38:14.885197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lem=[]\n",
    "for line in stop:\n",
    " words=[]\n",
    " for word in line:\n",
    "   words.append(word_net_lem.lemmatize(word))\n",
    " lem.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1560afb4",
   "metadata": {
    "papermill": {
     "duration": 0.013253,
     "end_time": "2023-02-07T02:38:16.986304",
     "exception": false,
     "start_time": "2023-02-07T02:38:16.973051",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here each of the characters is lemmatized using the lemmatize function of Lemmatizer and the lemmatized words are appended to the lem list created.\n",
    "\n",
    "**Step-4: Analyzing the lemmatized textual data**\n",
    "\n",
    "Let us analyze the lower case converted data along with the lower case converted data and the lemmatized data for better interpretation between stemming and lemmatization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b07c85e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T02:38:17.016376Z",
     "iopub.status.busy": "2023-02-07T02:38:17.015808Z",
     "iopub.status.idle": "2023-02-07T02:38:17.023534Z",
     "shell.execute_reply": "2023-02-07T02:38:17.022066Z"
    },
    "papermill": {
     "duration": 0.025793,
     "end_time": "2023-02-07T02:38:17.026210",
     "exception": false,
     "start_time": "2023-02-07T02:38:17.000417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data is : ['i', 'really', 'enjoyed', 'eating', 'here']\n",
      "\n",
      "Stemmed data is : ['real', 'enjoy', 'eat']\n",
      "\n",
      "Lemmatized data is : ['really', 'enjoyed', 'eating']\n"
     ]
    }
   ],
   "source": [
    "print('Original data is : {}'.format(lower[-1]))\n",
    "print()\n",
    "print('Stemmed data is : {}'.format(stem[-1]))\n",
    "print()\n",
    "print('Lemmatized data is : {}'.format(lem[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83500ea",
   "metadata": {
    "papermill": {
     "duration": 0.013307,
     "end_time": "2023-02-07T02:38:17.054144",
     "exception": false,
     "start_time": "2023-02-07T02:38:17.040837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.354935,
   "end_time": "2023-02-07T02:38:18.091601",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-07T02:37:59.736666",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
